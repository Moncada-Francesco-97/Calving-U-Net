{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import fiona\n",
    "import sklearn\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the selection is:  (128,)\n"
     ]
    }
   ],
   "source": [
    "#Upload the selection of glaccier i will use in the model\n",
    "selection = []\n",
    "\n",
    "with open('/Users/francesco/Desktop/Thesis/Data/dataset_filtered' + '/selecao.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        selection.append(int(line.strip()))\n",
    "\n",
    "common_years = np.arange(2005,2017,1)\n",
    "\n",
    "print('The shape of the selection is: ', np.shape(selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'int:10', 'name': 'str:30', 'regions': 'str:11', 'type': 'str:2', 'is_index': 'int:10', 'ice_shelf_': 'str:50', 'measures_n': 'str:50', 'latitude': 'float:24.15', 'longitude': 'float:24.15', 'thickness_': 'float:24.15', 'thicknes_1': 'float:24.15', 'smb_mean_9': 'float:24.15', 'smb_mean_1': 'float:24.15', 'ss_gl_flux': 'float:24.15', 'ss_gl_fl_1': 'float:24.15', 'ss_calving': 'float:24.15', 'ss_calvi_1': 'float:24.15', 'model_gl_f': 'float:24.15', 'instant_co': 'float:24.15', 'mass_loss_': 'float:24.15', 'mass_los_1': 'float:24.15', 'mass_los_2': 'float:24.15', 'mass_los_3': 'float:24.15', 'mass_los_4': 'float:24.15', 'mass_los_5': 'float:24.15', 'mass_los_6': 'float:24.15', 'mass_los_7': 'float:24.15', 'mass_los_8': 'float:24.15', 'mass_los_9': 'float:24.15', 'mass_los10': 'float:24.15', 'mass_los11': 'float:24.15', 'mass_los12': 'float:24.15', 'mass_los13': 'float:24.15', 'mass_los14': 'float:24.15', 'mass_los15': 'float:24.15', 'mass_los16': 'float:24.15', 'mass_los17': 'float:24.15', 'mass_los18': 'float:24.15', 'mass_los19': 'float:24.15', 'mass_los20': 'float:24.15', 'mass_los21': 'float:24.15', 'mass_los22': 'float:24.15', 'mass_los23': 'float:24.15', 'mass_los24': 'float:24.15', 'mass_los25': 'float:24.15', 'mass_los26': 'float:24.15', 'mass_los27': 'float:24.15', 'mass_los28': 'float:24.15', 'mass_los29': 'float:24.15', 'mass_los30': 'float:24.15', 'mass_los31': 'float:24.15', 'mass_los32': 'float:24.15', 'mass_los33': 'float:24.15', 'mass_los34': 'float:24.15', 'mass_los35': 'float:24.15', 'mass_los36': 'float:24.15', 'mass_los37': 'float:24.15', 'mass_los38': 'float:24.15', 'mass_los39': 'float:24.15', 'mass_los40': 'float:24.15', 'mass_los41': 'float:24.15', 'mass_los42': 'float:24.15', 'mass_los43': 'float:24.15', 'mass_los44': 'float:24.15', 'mass_los45': 'float:24.15', 'mass_los46': 'float:24.15', 'mass_los47': 'float:24.15', 'mass_los48': 'float:24.15', 'rel97_mass': 'float:24.15', 'rel97_ma_1': 'float:24.15', 'rel97_ma_2': 'float:24.15', 'rel97_ma_3': 'float:24.15', 'rel97_ma_4': 'float:24.15', 'rel97_ma_5': 'float:24.15', 'rel97_ma_6': 'float:24.15', 'rel97_ma_7': 'float:24.15', 'rel97_ma_8': 'float:24.15', 'rel97_ma_9': 'float:24.15', 'rel97_ma10': 'float:24.15', 'rel97_ma11': 'float:24.15', 'rel97_ma12': 'float:24.15', 'rel97_ma13': 'float:24.15', 'rel97_ma14': 'float:24.15', 'rel97_ma15': 'float:24.15', 'rel97_ma16': 'float:24.15', 'rel97_ma17': 'float:24.15', 'rel97_ma18': 'float:24.15', 'rel97_ma19': 'float:24.15', 'rel97_ma20': 'float:24.15', 'rel97_ma21': 'float:24.15', 'rel97_ma22': 'float:24.15', 'rel97_ma23': 'float:24.15', 'rel97_ma24': 'float:24.15', 'rel97_ma25': 'float:24.15', 'rel97_ma26': 'float:24.15', 'rel97_ma27': 'float:24.15', 'rel97_ma28': 'float:24.15', 'rel97_ma29': 'float:24.15', 'rel97_ma30': 'float:24.15', 'rel97_ma31': 'float:24.15', 'rel97_ma32': 'float:24.15', 'rel97_ma33': 'float:24.15', 'rel97_ma34': 'float:24.15', 'rel97_ma35': 'float:24.15', 'rel97_ma36': 'float:24.15', 'rel97_ma37': 'float:24.15', 'rel97_ma38': 'float:24.15', 'rel97_ma39': 'float:24.15', 'rel97_ma40': 'float:24.15', 'rel97_ma41': 'float:24.15', 'rel97_ma42': 'float:24.15', 'rel97_ma43': 'float:24.15', 'rel97_ma44': 'float:24.15', 'rel97_ma45': 'float:24.15', 'rel97_ma46': 'float:24.15', 'rel_mass_l': 'float:24.15', 'area': 'int:10', 'area_fra': 'int:12'}\n"
     ]
    }
   ],
   "source": [
    "path_to_tif = '/Users/francesco/Desktop/Data/GEOTIFFs/'\n",
    "shapefile_path = '/Users/francesco/Desktop//Thesis/Data/ice_shelf.shp'\n",
    "\n",
    "ids = []\n",
    "Names = []\n",
    "regions = []\n",
    "areas = []\n",
    "lats = []\n",
    "lons = []\n",
    "\n",
    "shapefile=fiona.open(shapefile_path)\n",
    "print(shapefile.schema['properties'])\n",
    "\n",
    "for feature in shapefile:\n",
    "\n",
    "    id = feature['properties']['id']\n",
    "    name=feature['properties']['name']\n",
    "    area=feature['properties']['area_fra']\n",
    "    lat = feature['properties']['latitude']\n",
    "    lon = feature['properties']['longitude']\n",
    "    geometry=feature['geometry']\n",
    "    if geometry is None:\n",
    "        continue\n",
    "    region = feature['properties']['regions']\n",
    "\n",
    "    #Taking the info\n",
    "    ids.append(id)\n",
    "    Names.append(name)\n",
    "    regions.append(region)\n",
    "    areas.append(area)\n",
    "    lats.append(lat)\n",
    "    lons.append(lon)\n",
    "\n",
    "#Creating the dataframe\n",
    "df_region = pd.DataFrame(index=ids)\n",
    "df_region ['id'] = ids\n",
    "df_region ['Name'] = Names\n",
    "df_region ['Region'] = regions\n",
    "df_region ['Area'] = areas\n",
    "df_region ['Latitude'] = lats\n",
    "df_region ['Longitude'] = lons\n",
    "\n",
    "#Selecting the region and sorting the dataframe\n",
    "df_region = df_region[df_region['id'].isin(selection)]\n",
    "df_region = df_region.sort_values(by=['id'])\n",
    "#drop id column\n",
    "df_region = df_region.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "dataset_directory = '/Users/francesco/Desktop/Thesis/Data/dataset_filtered/'\n",
    "\n",
    "# Basal Melting\n",
    "bm = pd.read_csv(dataset_directory + '/bm.csv', index_col=0)\n",
    "bm = bm.sort_values(by=['id']) #sorting the glaciers by their index\n",
    "bm = bm.loc[bm.index.isin(selection)] #selecting the glaciers, according to their index\n",
    "bm = bm[common_years.astype(str)] #selecting the common years\n",
    "bm = bm.sort_index() #sorting the glaciers by their index\n",
    "\n",
    "#Load the calving data\n",
    "calving = pd.read_csv(dataset_directory+ '/df_calving_from_shp_negative_and_positive.csv', index_col=0)\n",
    "calving = calving.loc[calving.index.isin(selection)]\n",
    "calving = calving[common_years.astype(str)]\n",
    "#sort the calving by its index\n",
    "calving = calving.sort_index()\n",
    "\n",
    "#Load the ice concentration data\n",
    "i_c = pd.read_csv(dataset_directory + '/ice_c_avg_extended_front.csv', index_col=0)\n",
    "i_c = i_c.loc[i_c.index.isin(selection)]\n",
    "i_c = i_c[common_years.astype(str)]\n",
    "i_c = i_c.sort_index()\n",
    "\n",
    "#Load the ice velocity data\n",
    "i_v = pd.read_csv(dataset_directory + '/velocity_80_percentile_extended_front_2011_2012_linear_trend.csv', index_col=0)\n",
    "i_v = i_v.loc[i_v.index.isin(selection)]\n",
    "i_v = i_v[common_years.astype(str)]\n",
    "i_v = i_v.sort_index()\n",
    "\n",
    "#Load the ice thickness data\n",
    "i_t = pd.read_csv(dataset_directory + '/thickness_avg_extended front.csv', index_col=0)\n",
    "i_t = i_t.loc[i_t.index.isin(selection)]\n",
    "i_t = i_t[common_years.astype(str)]\n",
    "i_t = i_t.sort_index()\n",
    "\n",
    "index = bm.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a dataset with all the variables\n",
    "dataset = pd.concat([bm, calving, i_c, i_v, i_t], axis=1, keys=['bm', 'calving', 'i_c', 'i_v', 'i_t'])\n",
    "dataset.columns.names = ['Variables', 'Years']\n",
    "dataset.index.names = ['Glaciers']\n",
    "\n",
    "#i want to split the dataset in two, one with the variables and one with the target. The target is ice calving\n",
    "dataset_target = dataset['calving']\n",
    "dataset_variables = dataset.drop(['calving'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folders division\n",
    "\n",
    "folder_1 = [2,4,6,7,8,15,18,16,34,45,135,163,95]\n",
    "folder_2 = [3,29,10,12,31,30,19,24,35,50,86,104,139]\n",
    "folder_3 = [65,43,37,17,41,32,38,40,36,52,143,146,125]\n",
    "folder_4 = [69,67,63,26,46,33,64,44,70,53,123,145,170]\n",
    "folder_5 = [75,78,89,39,47,81,77,54,71,55,118,101,117]\n",
    "folder_6 = [76,80,114,58,48,84,83,66,90,56,140,144,164]\n",
    "folder_7 = [88,82,120,61,57,92,85,105,100,73,158,124,169]\n",
    "folder_8 = [96,107,121,62,60,127,87,108,102,91,161,122,160]\n",
    "\n",
    "folder_9 = [129,112,156,68,97,132,93,109,115,98,148,166] #test\n",
    "folder_10 = [131,147,157,72,99,136,116,111,119,110,162,159] #test\n",
    "\n",
    "#merge the folders, in order to have a single list\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "train = [folder_1, folder_2, folder_3, folder_4, folder_5, folder_6, folder_7, folder_8]\n",
    "test = [folder_9, folder_10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Operate block division\n",
    "cv_block_1 = dataset.loc[folder_1]\n",
    "cv_block_2 = dataset.loc[folder_2]\n",
    "cv_block_3 = dataset.loc[folder_3]\n",
    "cv_block_4 = dataset.loc[folder_4]\n",
    "cv_block_5 = dataset.loc[folder_5]\n",
    "cv_block_6 = dataset.loc[folder_6]\n",
    "cv_block_7 = dataset.loc[folder_7]\n",
    "cv_block_8 = dataset.loc[folder_8]\n",
    "\n",
    "cv_blocks = [cv_block_1, cv_block_2, cv_block_3, cv_block_4, cv_block_5, cv_block_6, cv_block_7, cv_block_8]\n",
    "\n",
    "#Selecting the variables\n",
    "cv_block_1_variables = cv_block_1.drop(['calving'], axis=1)\n",
    "cv_block_2_variables = cv_block_2.drop(['calving'], axis=1)\n",
    "cv_block_3_variables = cv_block_3.drop(['calving'], axis=1)\n",
    "cv_block_4_variables = cv_block_4.drop(['calving'], axis=1)\n",
    "cv_block_5_variables = cv_block_5.drop(['calving'], axis=1)\n",
    "cv_block_6_variables = cv_block_6.drop(['calving'], axis=1)\n",
    "cv_block_7_variables = cv_block_7.drop(['calving'], axis=1)\n",
    "cv_block_8_variables = cv_block_8.drop(['calving'], axis=1)\n",
    "\n",
    "cv_block_variables = [cv_block_1_variables, cv_block_2_variables, cv_block_3_variables, cv_block_4_variables, cv_block_5_variables, cv_block_6_variables, cv_block_7_variables, cv_block_8_variables]\n",
    "\n",
    "#Selecting the target\n",
    "cv_block_1_target = cv_block_1['calving']\n",
    "cv_block_2_target = cv_block_2['calving']\n",
    "cv_block_3_target = cv_block_3['calving']\n",
    "cv_block_4_target = cv_block_4['calving']\n",
    "cv_block_5_target = cv_block_5['calving']\n",
    "cv_block_6_target = cv_block_6['calving']\n",
    "cv_block_7_target = cv_block_7['calving']\n",
    "cv_block_8_target = cv_block_8['calving']\n",
    "\n",
    "cv_block_targets = [cv_block_1_target, cv_block_2_target, cv_block_3_target, cv_block_4_target, cv_block_5_target, cv_block_6_target, cv_block_7_target, cv_block_8_target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to immplement the cross validation in blocks\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "#rf = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "best_parameters = []\n",
    "\n",
    "\n",
    "for i in range(len(cv_blocks)):\n",
    "    print(f\"Fold {i +1 }:\")\n",
    "\n",
    "    #Extracting the training an testing blocks from the list\n",
    "    test_block = cv_blocks[i]\n",
    "    train_blocks = cv_blocks[:i] + cv_blocks[i+1:]\n",
    "\n",
    "    #Extracting the training and testing variables and targets\n",
    "    X_test = test_block.drop(['calving'], axis=1)\n",
    "    y_test = test_block['calving']\n",
    "\n",
    "    X_train = pd.concat(train_blocks).drop(['calving'], axis=1)\n",
    "    y_train = pd.concat(train_blocks)['calving']\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    xgb_model_cv = GridSearchCV(xgb.XGBRegressor(), grid)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    xgb_model_cv.fit(X_train, y_train)\n",
    "    y_pred = xgb_model_cv.predict(X_test)\n",
    "\n",
    "    '''\n",
    "    plt.scatter(y_test, y_pred)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predictions')\n",
    "\n",
    "    #plot the line y= x (diagonal of the first and third quadrant)\n",
    "    max_value = np.max([y_test, y_pred])\n",
    "    min_value = np.min([y_test, y_pred])\n",
    "    x = np.linspace(min_value, max_value, 100)\n",
    "    plt.plot(x, x, color='black')\n",
    "    plt.title('Fold ' + str(i+1))\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    '''\n",
    "\n",
    "    # Evaluate the model's performance\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mae_scores.append(mae)\n",
    "\n",
    "    best_parameters.append(rf_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for the block  0  is:  [  -10.64023056 -2756.96996508 -6791.06483503 -4248.16741749\n",
      "   -40.67776316]\n",
      "The score for the block  1  is:  [-2097.97989868    -3.72819618 -2172.15697605 -3006.25288465\n",
      "   -32.6781086 ]\n",
      "The score for the block  2  is:  [-73.82074988  -5.85317016 -96.42872844 -55.05891223 -38.01497757]\n",
      "The score for the block  3  is:  [-1.32432348e+01 -8.06455971e+01 -4.07830935e+03 -2.08674670e+02\n",
      " -2.03173624e+00]\n",
      "The score for the block  4  is:  [-6.34946085e+00 -5.67810557e+00 -6.70933302e+03 -1.01090205e+04\n",
      " -2.05889142e+01]\n",
      "The score for the block  5  is:  [   -67.40371266 -26726.20334862   -173.01548106 -40676.53896089\n",
      "   -115.63300012]\n",
      "The score for the block  6  is:  [ -30.13754018 -337.78433294 -350.12452017  -99.08480173  -57.6123823 ]\n",
      "The score for the block  7  is:  [ -8674.12965997  -8487.02351694   -346.33354902 -12407.00786592\n",
      "   -383.13696689]\n"
     ]
    }
   ],
   "source": [
    "#perform the cross validation with xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "#Create the parameters for the model\n",
    "params = {\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'n_estimators': 1000\n",
    "}\n",
    "\n",
    "#Create the model\n",
    "model = xgb.XGBRegressor(**params)\n",
    "\n",
    "#Create the cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Create the cross validation\n",
    "scores = []\n",
    "for i in range(len(cv_blocks)):\n",
    "    score = cross_val_score(model, cv_block_variables[i], cv_block_targets[i], scoring='neg_mean_squared_error')\n",
    "    scores.append(score)\n",
    "    print('The score for the block ', i, ' is: ', score)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
